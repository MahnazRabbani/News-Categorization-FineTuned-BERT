{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT\n",
    "\n",
    "In this section, we initially provide a brief overview of BERT and the process of fine-tuning it. And then:   \n",
    "- we execute the fine-tuning of a pretrained BERT model on the 'ag_news' dataset, a process known as domain adaptation.     \n",
    "- we compare the accuracy of this model with a BERT model that has been fine-tuned specifically for the classification task at hand.     \n",
    "- we proceed furthur to fine-tune the domain-adapted model on the specific task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT or Bidirectional Encoder Representations from Transformers    \n",
    "\n",
    "BERT or Bidirectional Encoder Representations from Transformers, is a deep learning model developed by Google for natural language processing (NLP) tasks in 2018 and was a major breakthrough.   \n",
    "\n",
    "BERT has a **transformer architecture**, a specific type of deep learning model that uses **self-attention** mechanisms. The transformer model **learns contextual relationships** between words in a text. In contrast to previous models such as LSTM (Long Short-Term Memory) that read text input sequentially (either from left-to-right or right-to-left), **BERT reads the entire sequence of words at once**, which is why it's considered **bidirectional**.    \n",
    "\n",
    "This bidirectional approach allows BERT to understand the context and meaning of a word based on all of its surroundings (left and right of the word). For example, in the sentence \"I picked up a pen to write\", the word \"write\" informs the model about the meaning of \"pen\". This feature makes BERT particularly effective for NLP tasks that require understanding context, including sentiment analysis, named entity recognition, and question answering among others.     \n",
    "\n",
    "BERT is trained on a large amount of text data, and it uses **two types of training strategies**:\n",
    "\n",
    "- **Masked Language Model (MLM)**: In this strategy, some percentage of the input words are masked (hidden) at random, and the model is trained to predict those masked words based on the context provided by the non-masked words.\n",
    "\n",
    "- **Next Sentence Prediction (NSP)**: In this strategy, the model is trained to predict whether one sentence follows another in a given text, learning to understand the relationship between sentences.\n",
    "\n",
    "After this pretraining, a BERT model can be **fine-tuned on a specific task with a smaller amount of data** because it has already learned useful representations of language from the pretraining stage. This fine-tuning is done by adding an extra output layer that matches the task, and then training the entire model on the specific task.      \n",
    "\n",
    "Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on Task\n",
    "\n",
    "For many NLP applications involving Transformer models, you can simply take a pretrained BERT and fine-tune it directly on your data **for the task at hand**. For exammple here we use it for News classification task with our labeled data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on dataset (domain adaptation)\n",
    "\n",
    "There are certain instances where it's preferable to first adjust the language models based on your data, prior to training a task-specific head. For instance, if your dataset comprises legal contracts or scientific articles, a standard Transformer model such as BERT may often treat the domain-specific words in your corpus as infrequent tokens, leading to possibly subpar performance. By fine-tuning the language model on data from the same domain, you can enhance the performance of numerous downstream tasks. This implies that you typically only need to do this step once! This process of **fine-tuning** a pretrained language model on **in-domain data** is usually called **domain adaptation**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a pretrained BERT model on the 'ag_news' dataset using masked language modeling (domain adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/Users/mahnaz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 154.40it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 125/125 [26:03<00:00, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1563.3703, 'train_samples_per_second': 0.64, 'train_steps_per_second': 0.08, 'train_loss': 2.811888916015625, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=2.811888916015625, metrics={'train_runtime': 1563.3703, 'train_samples_per_second': 0.64, 'train_steps_per_second': 0.08, 'train_loss': 2.811888916015625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load the dataset\n",
    "raw_datasets = load_dataset(\"ag_news\")\n",
    "train_subset = raw_datasets['train'].select(range(1000))\n",
    "texts = [example['text'] for example in train_subset]\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "# Prepare for masked language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "# Convert inputs to a Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = MyDataset(inputs)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Specify the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./MyBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=100,  # Decrease this if your dataset is small\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a pretrained BERT model specifically for the classification task:\n",
    "\n",
    "The fine tuning of BERT on task can be found in initial_training.ipynb."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the domain-adapted model on the specific task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/Users/mahnaz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 416.39it/s]\n",
      "Some weights of the model checkpoint at ./MyBERT/checkpoint-100 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./MyBERT/checkpoint-100 and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/2n/hm1qbpc51xqfq4d48nhrz5q80000gn/T/ipykernel_82847/2422848234.py:30: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return load_metric(metric_name)\n",
      "/Users/mahnaz/vscodeProjects/News-Categorization-FineTuned-BERT/venv_News-Categorization/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 11%|█         | 69/625 [19:10<2:15:38, 14.64s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "def load_data_and_model(dataset_name, model_name, num_labels):\n",
    "    raw_datasets = load_dataset(dataset_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # Load the original tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels) # Load fine-tuned model\n",
    "    return raw_datasets, tokenizer, model\n",
    "\n",
    "\n",
    "def tokenize_data(raw_datasets, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)) \n",
    "    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000)) \n",
    "    full_train_dataset = tokenized_datasets[\"train\"]\n",
    "    full_eval_dataset = tokenized_datasets[\"test\"]\n",
    "    \n",
    "    return small_train_dataset, small_eval_dataset, full_train_dataset, full_eval_dataset\n",
    "\n",
    "def get_training_args(path, evaluation_strategy):\n",
    "    return TrainingArguments(path, evaluation_strategy=evaluation_strategy)\n",
    "\n",
    "def get_metric(metric_name):\n",
    "    return load_metric(metric_name)\n",
    "\n",
    "def compute_metrics(eval_pred, metric):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def get_trainer(model, args, train_dataset, eval_dataset, compute_metrics):\n",
    "    return Trainer(model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics)\n",
    "\n",
    "def evaluate_model(trainer):\n",
    "    return trainer.evaluate()\n",
    "\n",
    "def save_model(trainer, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    trainer.save_model(model_path)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load your fine-tuned model\n",
    "        \n",
    "        raw_datasets, tokenizer, model = load_data_and_model(\"ag_news\", \"./MyBERT/checkpoint-100\", 4)\n",
    "    \n",
    "        small_train_dataset, small_eval_dataset, full_train_dataset, full_eval_dataset = tokenize_data(raw_datasets, tokenizer)\n",
    "        \n",
    "        # Specify the number of epochs\n",
    "        training_args = get_training_args(\"test_trainer\", \"epoch\")\n",
    "        training_args.num_train_epochs = 5\n",
    "\n",
    "        metric = get_metric(\"accuracy\")\n",
    "\n",
    "        trainer = get_trainer(model, training_args, small_train_dataset, small_eval_dataset, lambda eval_pred: compute_metrics(eval_pred, metric))\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        eval_results = evaluate_model(trainer)\n",
    "        print(eval_results)\n",
    "\n",
    "        save_model(trainer, './models')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the diffrently fine-tuned BERT models \n",
    "\n",
    "The accuracy of the BERT fine-tuned on the task (find it in initial_training.ipynb) is 0.885. The accuracy of the BERT that is first fine-tuned on data and the fine-tuned on the task is  . This shows ...    \n",
    "\n",
    "Now that we stablished the models on a small sample of the dataset, we move forward and do the fine-tuning on the entire dataset which contains 120k samples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_News-Categorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2f579e51b388ca38e2e9f7d7a1e721a01ca5e6a35b59bd63e5175d6a356866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
