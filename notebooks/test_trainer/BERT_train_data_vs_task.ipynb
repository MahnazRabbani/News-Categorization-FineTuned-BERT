{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT or Bidirectional Encoder Representations from Transformers    \n",
    "\n",
    "BERT or Bidirectional Encoder Representations from Transformers, is a deep learning model developed by Google for natural language processing (NLP) tasks in 2018 and was a major breakthrough.   \n",
    "\n",
    "BERT has a **transformer architecture**, a specific type of deep learning model that uses **self-attention** mechanisms. The transformer model **learns contextual relationships** between words in a text. In contrast to previous models such as LSTM (Long Short-Term Memory) that read text input sequentially (either from left-to-right or right-to-left), **BERT reads the entire sequence of words at once**, which is why it's considered **bidirectional**.    \n",
    "\n",
    "This bidirectional approach allows BERT to understand the context and meaning of a word based on all of its surroundings (left and right of the word). For example, in the sentence \"I picked up a pen to write\", the word \"write\" informs the model about the meaning of \"pen\". This feature makes BERT particularly effective for NLP tasks that require understanding context, including sentiment analysis, named entity recognition, and question answering among others.     \n",
    "\n",
    "BERT is trained on a large amount of text data, and it uses **two types of training strategies**:\n",
    "\n",
    "- **Masked Language Model (MLM)**: In this strategy, some percentage of the input words are masked (hidden) at random, and the model is trained to predict those masked words based on the context provided by the non-masked words.\n",
    "\n",
    "- **Next Sentence Prediction (NSP)**: In this strategy, the model is trained to predict whether one sentence follows another in a given text, learning to understand the relationship between sentences.\n",
    "\n",
    "After this pretraining, a BERT model can be **fine-tuned on a specific task with a smaller amount of data** because it has already learned useful representations of language from the pretraining stage. This fine-tuning is done by adding an extra output layer that matches the task, and then training the entire model on the specific task.      \n",
    "\n",
    "Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning BERT on Task\n",
    "\n",
    "For many NLP applications involving Transformer models, you can simply take a pretrained BERT and fine-tune it directly on your data **for the task at hand**. For exammple here we use it for News classification task with our labeled data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning BERT on Data\n",
    "\n",
    "There are certain instances where it's preferable to first adjust the language models based on your data, prior to training a task-specific head. For instance, if your dataset comprises legal contracts or scientific articles, a standard Transformer model such as BERT may often treat the domain-specific words in your corpus as infrequent tokens, leading to possibly subpar performance. By fine-tuning the language model on data from the same domain, you can enhance the performance of numerous downstream tasks. This implies that you typically only need to do this step once! This process of **fine-tuning** a pretrained language model on **in-domain data** is usually called domain adaptation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_News-Categorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (main, Mar  8 2023, 04:44:36) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2f579e51b388ca38e2e9f7d7a1e721a01ca5e6a35b59bd63e5175d6a356866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
