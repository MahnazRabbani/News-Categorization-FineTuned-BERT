{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words and Naive Bayes Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is bag of words anyway?     \n",
    "\n",
    "The Bag of Words (BoW) method is a popular way to **represent text data** in machine learning, which treats **each document as an unordered collection** or \"bag\" of words. This method is used for feature extraction in text data. \n",
    "\n",
    "In the Bag of Words method, **a text** (such as a sentence or a document) **is represented as the bag (multiset) of its words**, **disregarding grammar** and even **word order** but **keeping multiplicity**. The **frequency of each word** is used as a **feature** for training a classifier.        \n",
    "\n",
    "## How does it work?       \n",
    "\n",
    "1. **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation. In this project our dadaset is already splited in sentences and is labeled.          \n",
    "\n",
    "2. **Vocabulary building**: All the words are collected and a dictionary is created where words are key and the indexes are values. The length of the dictionary is the length of the individual text representation.\n",
    "\n",
    "3. **Text to Vector**: Each sentence or document is converted into a vector of length equal to vocabulary. The presence of word from the vocabulary in the text will make the respective position in the vector 1, and if the word is absent the position will be 0. If 'n' number of times a word occurs in the text, the respective position's value will be 'n'.\n",
    "\n",
    "For example, let's consider two sentences:\n",
    "- Sentence 1: \"The cat sat on the mat.\"\n",
    "- Sentence 2: \"The dog sat on the log.\"\n",
    "\n",
    "In a BoW model, these sentences would first be tokenized into:\n",
    "\n",
    "- Sentence 1: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "- Sentence 2: [\"The\", \"dog\", \"sat\", \"on\", \"the\", \"log\"]\n",
    "\n",
    "Then, the vocabulary of unique words would be: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"dog\", \"log\"]\n",
    "\n",
    "Lastly, the sentences would be transformed into vectors based on this vocabulary:\n",
    "\n",
    "- Sentence 1: [2, 1, 1, 1, 1, 0, 0]\n",
    "- Sentence 2: [2, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "As you can see, each index in the vector corresponds to a word in the vocabulary, and the value at each index corresponds to the number of times that word appears in the sentence. \n",
    "\n",
    "The BoW approach is simple and effective, but it has some downsides. It creates sparse vectors because the length of the vector is the same as the length of the vocabulary, and for each sentence or document, many positions will be zero if the word is not present in the document. Also, this approach doesn't account for word order or context, so it might not be as effective for tasks where these elements are important."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the dataset, then, we convert the training data into a list of strings, and after that we pass the list to **CountVectorizer**. CountVectorizer transforms the text data into a bag of words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/Users/mahnaz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 326.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#reading the dataset\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:  dict_keys(['train', 'test'])\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n",
      "example of train datapoint: \n",
      " {'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# lets first have a look at the data\n",
    "print(\"keys: \",raw_datasets.keys())\n",
    "print(type(raw_datasets['train']))\n",
    "print(raw_datasets['train'].features)\n",
    "print(f\"example of train datapoint: \\n {raw_datasets['train'][0]}\")\n",
    "print(raw_datasets['train'][0]['text'])\n",
    "print(raw_datasets['train'][0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "{'text': 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.', 'label': 2}\n",
      "{'text': \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\", 'label': 2}\n",
      "{'text': 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.', 'label': 2}\n",
      "{'text': 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.', 'label': 2}\n",
      "{'text': 'Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\\\but stayed near lows for the year as oil prices surged past  #36;46\\\\a barrel, offsetting a positive outlook from computer maker\\\\Dell Inc. (DELL.O)', 'label': 2}\n",
      "{'text': \"Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\", 'label': 2}\n",
      "{'text': 'Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.', 'label': 2}\n",
      "{'text': 'Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.', 'label': 2}\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black  NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# printing the first 10 lines of the data:\n",
    "for i in range(10):\n",
    "    print(raw_datasets['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data into list of strings\n",
    "train_texts = [example['text'] for example in raw_datasets['train']]\n",
    "train_labels = [example['label'] for example in raw_datasets['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer transforms each text into a vector in a high-dimensional space. The dimension of this space is equal to the size of the learned vocabulary (i.e., the number of unique words in all documents of the training data). Each unique word has its own dimension and the value in that dimension is the count of this word in the corresponding document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a vocabulary dictionary of all tokens in the raw documents and return term-document matrix\n",
    "X_train = vectorizer.fit_transform(train_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (120000, 65006)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 62536)\t2\n",
      "  (0, 54842)\t1\n",
      "  (0, 7396)\t1\n",
      "  (0, 12600)\t1\n",
      "  (0, 6510)\t1\n",
      "  (0, 30178)\t1\n",
      "  (0, 57946)\t1\n",
      "  (0, 8345)\t1\n",
      "  (0, 48864)\t2\n",
      "  (0, 52479)\t1\n",
      "  (0, 51606)\t1\n",
      "  (0, 55636)\t1\n",
      "  (0, 18921)\t1\n",
      "  (0, 6832)\t1\n",
      "  (0, 40992)\t1\n",
      "  (0, 60125)\t1\n",
      "  (0, 15556)\t1\n",
      "  (0, 5306)\t1\n",
      "  (0, 51513)\t1\n",
      "  (0, 25524)\t1\n",
      "  (0, 3522)\t1\n"
     ]
    }
   ],
   "source": [
    "type(X_train)\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(type(X_train[0]))\n",
    "print(X_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "\n",
    "The above output is a representation of a sparse matrix in the Compressed Sparse Row (CSR) format. Let's break down the information:\n",
    "\n",
    "The type of the object (that is a matrix representation of a datapoint/sentence), is a sparse matrix in CSR format from the SciPy library (`<class 'scipy.sparse._csr.csr_matrix'>`)\n",
    "\n",
    "Following that, there are multiple lines with a specific format `(row_index, column_index) value`. These lines **represent the non-zero elements of the sparse matrix**. Here's an explanation of each line:\n",
    "\n",
    "- `(0, 7250) 2`: This line indicates that the value `2` is present at row index `0` and column index `7250`.        \n",
    "- `(0, 6270) 1`: This line indicates that the value `1` is present at row index `0` and column index `6270`.         \n",
    "- `(0, 738) 1`: This line indicates that the value `1` is present at row index `0` and column index `738`.          \n",
    "- ...         \n",
    "\n",
    "Each subsequent line follows the same pattern, representing the row index, column index, and value of a non-zero element in the sparse matrix.         \n",
    "\n",
    "In summary, this output is a **representation of a sparse matrix in CSR format**, where the **non-zero elements are shown with their corresponding row and column indices**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also convert labels to a numpy array for later use with Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train:  (120000,)\n",
      "<class 'numpy.int64'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(type(y_train[10]))\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: [0 1 2 3]\n",
      "Counts: [30000 30000 30000 30000]\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(\"Unique values:\", unique_values)\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try the Naive Bayes classifier with a small sample of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/Users/mahnaz/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 383.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: [0 1 2 3]\n",
      "Counts: [212 142 174 472]\n",
      "Accuracy: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "raw_datasets = load_dataset(\"ag_news\")\n",
    "\n",
    "# Select the first 1000 samples\n",
    "train_subset = raw_datasets['train'].select(range(1000))\n",
    "\n",
    "\n",
    "# Convert the train data into list of strings\n",
    "train_texts = train_subset['text']\n",
    "train_labels = train_subset['label']\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the train data into the BoW representation\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# Printing the counts of each class to make sure all classes have datapoints\n",
    "unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(\"Unique values:\", unique_values)\n",
    "print(\"Counts:\", counts)\n",
    "\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "naive_bayes_clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "naive_bayes_clf.fit(X_train, y_train)\n",
    "\n",
    "# Select the first 1000 samples from test set\n",
    "test_subset = raw_datasets['test'].select(range(1000))\n",
    "\n",
    "# Convert the test data into list of strings\n",
    "test_texts = test_subset['text']\n",
    "test_labels = test_subset['label']\n",
    "\n",
    "# Transform the test data into the BoW representation\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = naive_bayes_clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a 0.72 accuracy with Naive Bayes algorithm which shows weaker prediction power in compare to accuracy of 0.885 with fine-tuned BERT. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_News-Categorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2f579e51b388ca38e2e9f7d7a1e721a01ca5e6a35b59bd63e5175d6a356866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
